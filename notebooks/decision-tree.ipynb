{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f20d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy_indexed as npi\n",
    "from collections import namedtuple, Counter\n",
    "\n",
    "from sklearn import datasets\n",
    "from textwrap import dedent\n",
    "\n",
    "\n",
    "# def argmax(distribution):\n",
    "#     return max(distribution.items(), key=lambda x:x[1])[0]\n",
    "\n",
    "# def class_probablity(values):\n",
    "#     counts = Counter(values)\n",
    "#     total = sum(counts.values())\n",
    "#     distribution = {value:count/total for value,count in counts.items()}\n",
    "#     return distribution\n",
    "\n",
    "\n",
    "def all_splits(sorted_values):\n",
    "    return [(a+b)/2 for a,b in zip(sorted_values[0:-1], sorted_values[1:])]\n",
    "\n",
    "def class_probability(values):\n",
    "    '''from a list of class labels to a vector for label distributions'''\n",
    "    groups = npi.group_by(values)\n",
    "    total = len(values)\n",
    "#     distribution = {k:c/total for k,c in zip(groups.unique, groups.count)}\n",
    "#     return distribution\n",
    "    return groups.count / total ## groups.count is an np.array of counts, with key values sorted already\n",
    "\n",
    "def gini_index(target):\n",
    "    p = class_probability(target)\n",
    "    purity = 1-(p**2).sum()\n",
    "    return purity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5512b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067f0859",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "print(iris.data.shape, iris.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3dbc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "Split = namedtuple('Split', ['attr', 'value', 'loss', 'left', 'right'])\n",
    "Dataset = namedtuple('Dataset', ['data', 'target'])\n",
    "    \n",
    "class LeafNode:\n",
    "    def __init__(self, prediction=None, members=[], level=0):\n",
    "        self.prediction = prediction\n",
    "        self.members = members\n",
    "        self.level=level\n",
    "    def __repr__(self):\n",
    "        return dedent(f'''Leaf(prediction={self.prediction}, size={self.members.data.shape[0]})''')\n",
    "\n",
    "\n",
    "class SplitNode:\n",
    "    def __init__(\n",
    "        self, \n",
    "        split=None, \n",
    "        prediction=None, \n",
    "        left=None,\n",
    "        right=None,\n",
    "        level=None,\n",
    "    ):\n",
    "        self.split = split\n",
    "        self.prediction = prediction\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.level = level\n",
    "        \n",
    "    def __repr__(self):\n",
    "        indent = 2\n",
    "        return dedent(f'''\n",
    "Split(attr={self.split.attr}, threshold={self.split.value}\n",
    "{\" \"*indent*self.left.level}L={self.left}\n",
    "{\" \"*indent*self.right.level}R={self.right}'''.strip(\"\\n\"))\n",
    "        \n",
    "\n",
    "def find_best_split(dataset, attr_dimensions=range(0,3), criterion=gini_index):\n",
    "    data = dataset.data\n",
    "    target = dataset.target\n",
    "    \n",
    "    ## compute all attribute-split qualities\n",
    "    splits = []\n",
    "    for a in attr_dimensions:\n",
    "        ## sort by attribute\n",
    "        data_sorted = sorted(data, key=lambda d:d[a])\n",
    "        attr_values = [d[a] for d in data_sorted]\n",
    "        for split_point in all_splits(attr_values):\n",
    "            ## split data by split point\n",
    "            data_left = data[data[:,a]<split_point]\n",
    "            target_left = target[data[:,a]<split_point]\n",
    "            data_right = data[data[:,a]>=split_point]\n",
    "            target_right = target[data[:,a]>=split_point]\n",
    "            \n",
    "            loss_left = criterion(target_left)\n",
    "            loss_right = criterion(target_right)\n",
    "            loss = loss_left + loss_right\n",
    "            splits.append(Split(\n",
    "                attr=a, \n",
    "                value=split_point, \n",
    "                loss=loss, \n",
    "                left=Dataset(data_left, target_left),\n",
    "                right=Dataset(data_right, target_right),\n",
    "            ))\n",
    "        \n",
    "    ## find best split\n",
    "    split = min(splits, key=lambda s:s.loss)\n",
    "    return split\n",
    "\n",
    "\n",
    "def fit_decision_tree_classifier(dataset, criterion=gini_index, level=0, attr_dimensions=[]):\n",
    "    data = dataset.data\n",
    "    target = dataset.target\n",
    "    \n",
    "    if len(attr_dimensions)==0:\n",
    "        is_leaf = True\n",
    "    else:\n",
    "        best_split = find_best_split(dataset, attr_dimensions=attr_dimensions)\n",
    "        is_leaf = best_split.loss<0.1 or data.shape[0] <= 4\n",
    "    \n",
    "    if is_leaf:\n",
    "        g = npi.group_by(target)\n",
    "        per_class_counts = g.count\n",
    "        unique_class_labels = g.unique\n",
    "        majority_class = unique_class_labels[per_class_counts.argmax()]\n",
    "        return LeafNode(prediction=majority_class, members=dataset, level=level)\n",
    "    else:\n",
    "        attr_dimensions.remove(best_split.attr)\n",
    "        left_node = fit_decision_tree_classifier(\n",
    "            best_split.left, \n",
    "            level=level+1, \n",
    "            attr_dimensions=attr_dimensions, \n",
    "        )\n",
    "        right_node = fit_decision_tree_classifier(\n",
    "            best_split.right, \n",
    "            level=level+1, \n",
    "            attr_dimensions=attr_dimensions, \n",
    "        )\n",
    "        return SplitNode(split=best_split, left=left_node, right=right_node, level=level)\n",
    "    \n",
    "    \n",
    "\n",
    "def find_leaf(tree, data_point):\n",
    "    node = tree\n",
    "    while type(node) is SplitNode:\n",
    "        attr = node.split.attr\n",
    "        thresh = node.split.value\n",
    "        left = data_point[attr] < thresh\n",
    "        if left:\n",
    "            node = node.left\n",
    "        else:\n",
    "            node = node.right\n",
    "    return node\n",
    "\n",
    "\n",
    "def predict(tree, data):\n",
    "    ## for each data point, find the leaf node\n",
    "    ## report leaf node prediction\n",
    "    return np.array([\n",
    "        find_leaf(tree, d).prediction \n",
    "        for d in data\n",
    "    ])\n",
    "# data_point = dataset.data[0]\n",
    "# find_leaf(tree, data_point)\n",
    "# predict(tree, dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa5f827",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_attributes = iris.data.shape[1]\n",
    "\n",
    "# find_best_split(iris, attr_dimensions=range(n_attributes))\n",
    "dataset = Dataset(iris.data, iris.target)\n",
    "\n",
    "\n",
    "tree = fit_decision_tree_classifier(dataset, attr_dimensions=list(range(4)))\n",
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a6910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(tree, dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94d31c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
